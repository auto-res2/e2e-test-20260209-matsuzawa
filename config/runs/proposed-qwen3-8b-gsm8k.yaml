run_id: proposed-qwen3-8b-gsm8k
method: MFT-AutoCoT (Metamorphic + Falsification-Driven, Cost-Adaptive Auto-CoT)
model:
  name: Qwen3-8B
  precision: bf16
  max_tokens: 256
dataset:
  name: gsm8k
  split:
    train: train[:300]
    test: test[:200]
  preprocessing:
    max_length: 512
    answer_type: int
training:
  task: icl_demo_selection
  learning_rate: 1.0e-5
  batch_size: 1
  epochs: 1
  optimizer: adamw
  warmup_steps: 0
  generation:
    temperature_solve: 0.7
    temperature_paraphrase: 0.3
    temperature_audit: 0.0
    max_tokens: 256
  clustering:
    embed_model: all-MiniLM-L6-v2
    k_clusters: 8
    C_candidates_per_cluster: 5
  mft_autocot:
    S_modes_max: 4
    K_paraphrases_max: 2
    accept_p: 0.75
    reject_p: 0.40
    max_views: 18
    tau_score_threshold: 0.55
    critic_model: Qwen3-8B
    adaptive_stopping: true
optuna:
  n_trials: 20
  search_spaces:
    - param_name: k_clusters
      distribution_type: categorical
      choices: [6, 8, 10]
    - param_name: C_candidates_per_cluster
      distribution_type: categorical
      choices: [3, 5]
    - param_name: S_modes_max
      distribution_type: categorical
      choices: [3, 4]
    - param_name: K_paraphrases_max
      distribution_type: categorical
      choices: [1, 2, 3]
    - param_name: max_views
      distribution_type: categorical
      choices: [12, 18, 24]
    - param_name: accept_p
      distribution_type: uniform
      low: 0.65
      high: 0.9
    - param_name: reject_p
      distribution_type: uniform
      low: 0.2
      high: 0.55
    - param_name: tau_score_threshold
      distribution_type: uniform
      low: 0.35
      high: 0.75
    - param_name: temperature_solve
      distribution_type: uniform
      low: 0.4
      high: 0.9
